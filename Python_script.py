# -*- coding: utf-8 -*-
"""Assignment_2_Group_25.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HtQ4yTM06CYXCJPi8R86j9lh6vl8MDg5

# **Task 1: Profiling relational data**
"""

import pandas as pd
import numpy as np

df_cas = pd.read_csv('dft-road-casualty-statistics-casualty-2022.csv', header = 0,
                 quotechar = '"',sep = ",",
                 na_values = ['na', '-', '.', ''])
df_cas

'''
1. Number of unique values in a column
'''
print('The number of accidents is:', df_cas['accident_index'].nunique())

'''
2. The sum of rows
'''
print('The number of accidents where the severity of the accident was mild is:', np.sum(df_cas['casualty_severity'] == 1))

'''
3. The minimum value in a column
'''
df_dupl = df_cas[df_cas.age_of_casualty != -1] # -1 corresponds to a null value so these values should not be included
print('The age of the youngest person that had an accident in 2022 is:', df_dupl['age_of_casualty'].min())

'''
4. The maximum value in a column
'''
print('The age of the oldest person that had an accident in 2022 is:', df_cas['age_of_casualty'].max())

'''
5. The mean of a column
'''
print('The average age of people that had an accident in 2022 is:', np.mean(df_cas['age_of_casualty']))

'''
6. The median of a column
'''
print('The age that occurs at the center of the age column is:', np.median(df_cas['age_of_casualty']))

'''
7. The standard deviation of a column
'''
print('The standard deviation of the age at which the accidents happened is:', np.std(df_cas['age_of_casualty']))

'''
8. The number of null values in a column
'''
print('The number of accidents where the age was not known is:', pd.value_counts(df_cas['age_of_casualty'] == -1))

'''
9. The mode of a column
'''
df_cas['sex_of_casualty']=df_cas['sex_of_casualty'].replace(-1,0)
print('The sex that had the most accidents in 2022 is:', np.argmax(np.bincount(df_cas['sex_of_casualty'])))

'''
10. The correlation
'''
df_cas.corr()

"""# **Task 2: Entity resolution**

Part 1
"""

!pip install py_stringmatching

import pandas as pd
from py_stringmatching import similarity_measure as sm

acm_database = "ACM.csv"
dblp_database = "DBLP2.csv"
dblp_acm_mapping_database = "DBLP-ACM_perfectMapping.csv"

df_acm = pd.read_csv(acm_database, encoding='ISO-8859-1', header=0, quotechar='"', sep=",", na_values=['na', '-', '.', ''])
df_dblp = pd.read_csv(dblp_database, encoding='ISO-8859-1', header=0, quotechar='"', sep=",", na_values=['na', '-', '.', ''])
dblp_acm_mapping = pd.read_csv(dblp_acm_mapping_database, encoding='ISO-8859-1', header=0, quotechar='"', sep=",", na_values=['na', '-', '.', ''])

'''
a. Ignore the pub_id.
'''
df_acm = df_acm.iloc[:,1:]
df_dblp = df_dblp.iloc[:,1:]

'''
b & c Change all alphabetical characters into lowercase and convert multiple spaces to one.
'''
for col in df_acm:
  if df_acm[col].dtype == "object":
    df_acm[col] = df_acm[col].str.lower().replace(to_replace=r'\s+', value=' ', regex=True)

for col in df_dblp:
  if df_dblp[col].dtype == "object":
    df_dblp[col] = df_dblp[col].str.lower().replace(to_replace=r'\s+', value=' ', regex=True)


'''
e. Use Jaro similarity to compare the values in the authors field and compute.
'''
def mod_jaro_sim(s1, s2):
#If both strings are null, we consider them to be similar
    if  str(s1) == 'nan' and str(s2) == 'nan':
        return 1
#If only one string is null, we consider them completely different sim = 0
    if str(s1) == 'nan':
        return 0
    if str(s2) == 'nan':
        return 0
    jaro_sim = sm.jaro.Jaro()
    j_sim = jaro_sim.get_raw_score (s1, s2)
    return j_sim

'''
f. Use a modified version of the affine similarity that is scaled to the interval [0, 1] for the venue attribute.
'''
def aff_sim (s1, s2, open_gap = 1, gap_ext =0.1):
    aff_sim = sm.affine.Affine(gap_start = 1, gap_continuation = 0.1, \
                       sim_func = lambda s1, s2: (int(1 if s1 == s2 else 0)))

    return aff_sim.get_raw_score(s1, s2) / min(len(s1), len(s2))* (1 + gap_ext)

'''
d. Use Levenshtein similarity for comparing the values in the title attribute and compute the score.
g. Use Match (1) / Mismatch (0) for the year.
h. Use the rec_sim formula to combine the scores and compute the final score.
'''
def record_sim(rec1, rec2, threshold):

    lev_sim = sm.levenshtein.Levenshtein()
    jaro_sim = sm.jaro.Jaro()

    # Compute the similarity score
    s_t = lev_sim.get_sim_score(rec1[0], rec2[0])
    s_a = mod_jaro_sim(rec1[1], rec2[1])
    s_c = aff_sim (rec1[2], rec2[2], 1, 0.1)
    s_y = 1 if rec1[3] == rec2[3] else 0
    rec_sim = 0.4 * s_t + 0.2 * s_a + 0.2 * s_c + 0.2 * s_y

    print(s_c)
    # Check if the similarity between the two records is greater than the threshold or not
    if rec_sim > threshold:
        return True
    else: return False

'''
i. Report the records with rec_sim > 0.7 as duplicate records by storing the ids of both records in a list.
'''
similar_list = []

for acm_row in range(len(df_acm)):
  for dblp_row in range(len(df_dblp)):
    row_1 = df_acm.iloc[acm_row]
    row_2 = df_dblp.iloc[dblp_row]
    result = record_sim(row_1,row_2, 0.7)
    if result:
      similar_list.append([acm_row,dblp_row])

'''
j. In the table DBLP-ACM_perfectMapping.csv, you can find the actual mappings (the ids of the correct duplicate records). Compute the precision of this method by counting the number of duplicate records that you discovered correctly. That is, among all the reported similar records by your method, how many pairs exist in the file DBLP-ACM_perfectMapping.csv.
'''
df_acm = pd.read_csv(acm_database, encoding='ISO-8859-1', header=0, quotechar='"', sep=",", na_values=['na', '-', '.', ''])
df_dblp = pd.read_csv(dblp_database, encoding='ISO-8859-1', header=0, quotechar='"', sep=",", na_values=['na', '-', '.', ''])

pairs = []
for x in range(len(similar_list)):
    retVal = df_acm.id[x]
    retVal2 = df_dblp.id[x]
    pairs.append((retVal2,retVal))

df_corr = pd.DataFrame(pairs, columns=['candidate1', 'candidate2'])
df_corr['candidate1'] = df_corr['candidate1'].str.lower()
df_corr['candidate1'] = df_corr['candidate1'].str.replace(r'\s+', ' ', regex=True)
dblp_acm_mapping['idDBLP'] = dblp_acm_mapping['idDBLP'].str.lower()
dblp_acm_mapping['idDBLP'] = dblp_acm_mapping['idDBLP'].str.replace(r'\s+', ' ', regex=True)
dblp_acm_mapping

dblp_dict = dict(zip(dblp_acm_mapping.idDBLP, dblp_acm_mapping.idACM))

correctPairs = 0
wrongPairs = 0

for x in range(len(df_corr)):
    candidate1 = df_corr.candidate1[x]
    candidate2 = df_corr.candidate2[x]

    # Check if candidate1 is in dblp_dict
    if candidate1 in dblp_dict:
        if candidate2 == dblp_dict[candidate1]:
            correctPairs += 1
        else:
            wrongPairs += 1

print("The precision is:", correctPairs/len(df_corr))

'''
k. Record the running time of the method. You can observe that the program takes a long time to get the results. What can you do to reduce the running time? (Just provide clear discussion â€“ no need for implementing the ideas.)
'''
# The answer to this question can be found in the pdf file.

"""Part 2"""

import pandas as pd
import csv
import numpy as np
import requests
from io import StringIO
from IPython.display import display
from random import shuffle
from itertools import combinations

df_ACM = pd.read_csv(filepath_or_buffer = 'ACM.csv',
                 delimiter=',', doublequote=True, quotechar='"',
                 na_values = ['na', '-', '.', ''],
                 quoting=csv.QUOTE_ALL, encoding = "ISO-8859-1")
df_DBLP = pd.read_csv(filepath_or_buffer = 'DBLP2.csv',
                 delimiter=',', doublequote=True, quotechar='"',
                 na_values = ['na', '-', '.', ''],
                 quoting=csv.QUOTE_ALL, encoding = "ISO-8859-1")

'''
1. Concatenate the values in each record into one single string.
'''
df_ACM['sentences_A'] = df_ACM.apply(lambda x: ' '.join(map(str, x)), axis=1)
df_DBLP['sentences_B'] = df_DBLP.apply(lambda x: ' '.join(map(str, x)), axis=1)

'''
2. Change all alphabetical characters into lowercase.
'''
df_ACM['sentences_A'] = df_ACM['sentences_A'].str.lower()
df_DBLP['sentences_B'] = df_DBLP['sentences_B'].str.lower()

'''
3. Convert multiple spaces to one.
'''
df_ACM['sentences_A'] = df_ACM['sentences_A'].str.replace(r'\s+', ' ', regex=True)
df_DBLP['sentences_B'] = df_DBLP['sentences_B'].str.replace(r'\s+', ' ', regex=True)

'''
4. Combine the records from both tables into one big list as we did during the lab.
'''
sentences = df_ACM['sentences_A'].tolist()
sentences.extend(df_DBLP['sentences_B'].tolist())
print(f"the number of sentences (documents):{len(sentences)}")

'''
5. Use the functions in the tutorials from lab 5 to compute the shingles, the minhash signature and the similarity.
'''
def shingle(text: str, k: int)->set:
    shingle_set = []
    for i in range(len(text) - k+1):
        shingle_set.append(text[i:i+k])
    return set(shingle_set)
def build_vocab(shingle_sets: list)->dict:
    full_set = {item for set_ in shingle_sets for item in set_}
    vocab = {}
    for i, shingle in enumerate(list(full_set)):
        vocab[shingle] = i
    return vocab
def one_hot(shingles: set, vocab: dict):
    vec = np.zeros(len(vocab))
    for shingle in shingles:
        idx = vocab[shingle]
        vec[idx] = 1
    return vec

k = 3
shingles = []
for sentence in sentences:
    shingles.append(shingle(sentence,k))
vocab = build_vocab(shingles)
print(f"Number of vocabulary is:{len(vocab)}")
shingles_1hot = []
for shingle_set in shingles:
    shingles_1hot.append(one_hot(shingle_set,vocab))
shingles_1hot = np.stack(shingles_1hot)
shingles_1hot

def get_minhash_arr(num_hashes:int,vocab:dict):
    length = len(vocab.keys())
    arr = np.zeros((num_hashes,length))
    for i in range(num_hashes):
        permutation = np.random.permutation(len(vocab.keys())) + 1
        arr[i,:] = permutation.copy()
    return arr.astype(int)
def get_signature(minhash:np.ndarray, vector:np.ndarray):
    idx = np.nonzero(vector)[0].tolist()
    shingles = minhash[:,idx]
    signature = np.min(shingles,axis=1)
    return signature

def jaccard_similarity(set1, set2):
    intersection_size = len(set1.intersection(set2))
    union_size = len(set1.union(set2))
    return intersection_size / union_size if union_size != 0 else 0.0

def compute_signature_similarity(signature_1, signature_2):

    # Ensure the matrices have the same shape
    if signature_1.shape != signature_2.shape:
        raise ValueError("Both signature matrices must have the same shape.")
    # Count the number of rows where the two matrices agree
    agreement_count = np.sum(signature_1 == signature_2)
    # Calculate the similarity
    similarity = agreement_count / signature_2.shape[0]

    return similarity

minhash_arr =  get_minhash_arr(100,vocab)
signatures = []
for vector in shingles_1hot:
    signatures.append(get_signature(minhash_arr,vector))
signatures = np.stack(signatures)
signatures.shape

class LSH:
    buckets = []
    counter = 0

    def __init__(self, b: int):
        self.b = b
        for i in range(b):
            self.buckets.append({})

    def make_subvecs(self, signature: np.ndarray) -> np.ndarray:
        l = len(signature)
        assert l % self.b == 0
        r = int(l / self.b)
        subvecs = []
        for i in range(0, l, r):
            subvecs.append(signature[i:i+r])
        return np.stack(subvecs)

    def add_hash(self, signature: np.ndarray):
        subvecs = self.make_subvecs(signature).astype(str)
        for i, subvec in enumerate(subvecs):
            subvec = ','.join(subvec)
            if subvec not in self.buckets[i].keys():
                self.buckets[i][subvec] = []
            self.buckets[i][subvec].append(self.counter)
        self.counter += 1

    def check_candidates(self) -> set:
        candidates = []
        for bucket_band in self.buckets:
            keys = bucket_band.keys()
            for bucket in keys:
                hits = bucket_band[bucket]
                if len(hits) > 1:
                    candidates.extend(combinations(hits, 2))
        return set(candidates)

b = 10   # Number of buckets
lsh = LSH(b)
for signature in signatures:
    lsh.add_hash(signature)
candidate_pairs = lsh.check_candidates()
len(candidate_pairs)

'''
6. Extract the top 2224 candidates from the LSH algorithm, compare them to the actual mappings in the file DBLP-ACM_perfectMapping.csv and compute the precision of the method.
'''
df_perfect = pd.read_csv(filepath_or_buffer = 'DBLP-ACM_perfectMapping.csv',
                 delimiter=',', doublequote=True, quotechar='"',
                 na_values = ['na', '-', '.', ''],
                 quoting=csv.QUOTE_ALL, encoding = "ISO-8859-1")
df_perfect = df_perfect.astype(str)
df_perfect['idDBLP'] = df_perfect['idDBLP'].str.lower()
df_perfect['idDBLP'] = df_perfect['idDBLP'].str.replace(r'\s+', ' ', regex=True)


number_of_candidates = 2224
data = []
for candidate in list(candidate_pairs)[:number_of_candidates]:
    tokens = sentences[candidate[0]].split(" ", 1);
    tokens2 = sentences[candidate[1]].split(" ", 1);
    retVal = tokens[0]
    retVal2 = tokens2[0]
    data.append((retVal2,retVal))

df = pd.DataFrame(data, columns=['candidate1', 'candidate2'])
df

# Create dictionaries for quick lookups
dblp_dict = dict(zip(df_perfect.idDBLP, df_perfect.idACM))
acm_dict = dict(zip(df_perfect.idACM, df_perfect.idDBLP))

correctPairs = 0
wrongPairs = 0
for x in range(len(df)):
    candidate1 = df.candidate1[x]
    candidate2 = df.candidate2[x]

    # Check if candidate1 is in dblp_dict
    if candidate1 in dblp_dict:
        if candidate2 == dblp_dict[candidate1]:
            correctPairs += 1
        else:
            wrongPairs += 1

    # Check if candidate1 is in acm_dict
    elif candidate1 in acm_dict:
        if candidate2 == acm_dict[candidate1]:
            correctPairs += 1
        else:
            wrongPairs += 1

print("The precision is:", correctPairs/len(df))

'''
7. Record the running time of the method.
8. Compare the precision and the running time in Parts 1 and 2.
'''
# The answers to these questions can be found in the pdf file

"""# **Task 3: Data preparation**"""

import pandas as pd
import numpy as np

df_pid = pd.read_csv('diabetes.csv', header = 0,
                 quotechar = '"',sep = ",",
                 na_values = ['na', '-', '.', ''])
df_pid

'''
1. Compute the correlation between the different columns after removing the outcome column.
'''
df_corr = df_pid.copy()
df_corr.drop(columns= 'Outcome', axis=1, inplace=True) # remove the 'Outcome' column
corr_mat = df_corr.corr() # compute the correlation
corr_mat

'''
2. Remove the disguised values from the table. We need to remove the values that equal to 0 from columns BloodPressure, SkinThickness and BMI as these are missing values but they have been replaced by the value 0. Remove the value but keep the record (i.e.) change the value to null.
'''
# replace all 0 values in the following columns with NaN
df_corr['BloodPressure']=df_corr['BloodPressure'].replace(0,np.nan)
df_corr['SkinThickness']=df_corr['SkinThickness'].replace(0,np.nan)
df_corr['BMI']=df_corr['BMI'].replace(0,np.nan)
df_corr

'''
3. Fill the cells with null using the mean values of the records that have the same class label.
'''
df_MF = df_corr.copy()
# replace all NaN values with the mean of the column
df_MF['BloodPressure'] = df_MF['BloodPressure'].fillna(np.mean(df_MF['BloodPressure']))
df_MF['SkinThickness'] = df_MF['SkinThickness'].fillna(np.mean(df_MF['SkinThickness']))
df_MF['BMI'] = df_MF['BMI'].fillna(np.mean(df_MF['BMI']))
df_MF

'''
4. Compute the correlation between the different columns.
'''
corr_mat2 = df_MF.corr() # compute the correlation again
display(corr_mat)

print("   ")
display(corr_mat2)

"""**Compare the values from this step with the values in the first step (just mention the most important changes (if any)) and comment on your findings.**

As the changes are made in the BloodPressure, SkinThickness and BMI columns, we will only look at the differences of the values within those columns. We can see that within the BloodPressure column, the correlation values of Pregnancies, Glucose and Age increased, while the correlation values of Insulin and DiabetesPedigreeFunction decreased.

For the SkinThickness column, the values of Pregancies, Glucose, BMI and Age increased. The values of Insulin and DiabetesPedigreeFunction decreased. In the BMI column, the most important difference is the increase of the SkinThickness value.

Other values have also changed. However, the difference between these values were so small that they were not worth mentioning.



"""
